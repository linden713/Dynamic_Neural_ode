[
    {
        "label": "gym",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gym",
        "description": "gym",
        "detail": "gym",
        "documentation": {}
    },
    {
        "label": "spaces",
        "importPath": "gym",
        "description": "gym",
        "isExtraImport": true,
        "detail": "gym",
        "documentation": {}
    },
    {
        "label": "os,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.",
        "description": "os.",
        "detail": "os.",
        "documentation": {}
    },
    {
        "label": "pybullet",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pybullet",
        "description": "pybullet",
        "detail": "pybullet",
        "documentation": {}
    },
    {
        "label": "pybullet_data",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pybullet_data",
        "description": "pybullet_data",
        "detail": "pybullet_data",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "numpngw",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpngw",
        "description": "numpngw",
        "detail": "numpngw",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "odeint",
        "importPath": "torchdiffeq",
        "description": "torchdiffeq",
        "isExtraImport": true,
        "detail": "torchdiffeq",
        "documentation": {}
    },
    {
        "label": "PandaPushingEnv",
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "isExtraImport": true,
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "TARGET_POSE_FREE",
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "isExtraImport": true,
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "TARGET_POSE_OBSTACLES",
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "isExtraImport": true,
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "OBSTACLE_HALFDIMS",
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "isExtraImport": true,
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "OBSTACLE_CENTRE",
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "isExtraImport": true,
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "BOX_SIZE",
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "isExtraImport": true,
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "PandaPushingEnv",
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "isExtraImport": true,
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "TARGET_POSE_FREE",
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "isExtraImport": true,
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "TARGET_POSE_OBSTACLES",
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "isExtraImport": true,
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "BOX_SIZE",
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "isExtraImport": true,
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "PandaPushingEnv",
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "isExtraImport": true,
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "process_data_single_step",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "process_data_multiple_step",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "SingleStepDynamicsDataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "MultiStepDynamicsDataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "process_data_multiple_step",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "random_split",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.notebook",
        "description": "tqdm.notebook",
        "isExtraImport": true,
        "detail": "tqdm.notebook",
        "documentation": {}
    },
    {
        "label": "display",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "ResidualDynamicsModel",
        "importPath": "model.nn_dynamics_models",
        "description": "model.nn_dynamics_models",
        "isExtraImport": true,
        "detail": "model.nn_dynamics_models",
        "documentation": {}
    },
    {
        "label": "ResidualDynamicsModel",
        "importPath": "model.nn_dynamics_models",
        "description": "model.nn_dynamics_models",
        "isExtraImport": true,
        "detail": "model.nn_dynamics_models",
        "documentation": {}
    },
    {
        "label": "NeuralODEDynamicsModel",
        "importPath": "model.neural_ODE_dynamics_models",
        "description": "model.neural_ODE_dynamics_models",
        "isExtraImport": true,
        "detail": "model.neural_ODE_dynamics_models",
        "documentation": {}
    },
    {
        "label": "NeuralODEDynamicsModel",
        "importPath": "model.neural_ODE_dynamics_models",
        "description": "model.neural_ODE_dynamics_models",
        "isExtraImport": true,
        "detail": "model.neural_ODE_dynamics_models",
        "documentation": {}
    },
    {
        "label": "GIFVisualizer",
        "importPath": "gif.visualizers",
        "description": "gif.visualizers",
        "isExtraImport": true,
        "detail": "gif.visualizers",
        "documentation": {}
    },
    {
        "label": "PushingController",
        "importPath": "controller",
        "description": "controller",
        "isExtraImport": true,
        "detail": "controller",
        "documentation": {}
    },
    {
        "label": "free_pushing_cost_function",
        "importPath": "controller",
        "description": "controller",
        "isExtraImport": true,
        "detail": "controller",
        "documentation": {}
    },
    {
        "label": "collision_detection",
        "importPath": "controller",
        "description": "controller",
        "isExtraImport": true,
        "detail": "controller",
        "documentation": {}
    },
    {
        "label": "obstacle_avoidance_pushing_cost_function",
        "importPath": "controller",
        "description": "controller",
        "isExtraImport": true,
        "detail": "controller",
        "documentation": {}
    },
    {
        "label": "functools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "functools",
        "description": "functools",
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "MultivariateNormal",
        "importPath": "torch.distributions.multivariate_normal",
        "description": "torch.distributions.multivariate_normal",
        "isExtraImport": true,
        "detail": "torch.distributions.multivariate_normal",
        "documentation": {}
    },
    {
        "label": "MultiStepLoss",
        "importPath": "model.losses",
        "description": "model.losses",
        "isExtraImport": true,
        "detail": "model.losses",
        "documentation": {}
    },
    {
        "label": "SE2PoseLoss",
        "importPath": "model.losses",
        "description": "model.losses",
        "isExtraImport": true,
        "detail": "model.losses",
        "documentation": {}
    },
    {
        "label": "torch.optim",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.optim",
        "description": "torch.optim",
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "PandaPushingEnv",
        "kind": 6,
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "peekOfCode": "class PandaPushingEnv(gym.Env): \n    def __init__(self, debug=False, visualizer=None, include_obstacle=True, render_non_push_motions=True,\n                 render_every_n_steps=1, camera_heigh=84, camera_width=84):\n        self.debug = debug\n        self.visualizer = visualizer\n        self.include_obstacle = include_obstacle\n        self.render_every_n_steps = render_every_n_steps\n        if debug:\n            p.connect(p.GUI)\n        else:",
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "quaternion_matrix",
        "kind": 2,
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "peekOfCode": "def quaternion_matrix(quaternion):\n    \"\"\"Return homogeneous rotation matrix from quaternion.\n1176\n1177      >>> R = quaternion_matrix([0.06146124, 0, 0, 0.99810947])\n1178      >>> numpy.allclose(R, rotation_matrix(0.123, (1, 0, 0)))\n1179      True\n1180\n1181      \"\"\"\n    q = np.array(quaternion[:4], dtype=np.float64, copy=True)\n    nq = np.dot(q, q)",
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "hw_dir",
        "kind": 5,
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "peekOfCode": "hw_dir = os.path.dirname(os.path.abspath(__file__))\nassets_dir = os.path.join(hw_dir, 'assets')\nBOX_SIZE = 0.1\nTARGET_POSE_FREE = np.array([0.8, 0, 0.])\nTARGET_POSE_OBSTACLES = np.array([0.8, -0.1, 0.])\nOBSTACLE_CENTRE = np.array([0.6, 0.2, 0.])\nOBSTACLE_HALFDIMS = np.array([0.05, 0.25, 0.05])\nclass PandaPushingEnv(gym.Env): \n    def __init__(self, debug=False, visualizer=None, include_obstacle=True, render_non_push_motions=True,\n                 render_every_n_steps=1, camera_heigh=84, camera_width=84):",
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "assets_dir",
        "kind": 5,
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "peekOfCode": "assets_dir = os.path.join(hw_dir, 'assets')\nBOX_SIZE = 0.1\nTARGET_POSE_FREE = np.array([0.8, 0, 0.])\nTARGET_POSE_OBSTACLES = np.array([0.8, -0.1, 0.])\nOBSTACLE_CENTRE = np.array([0.6, 0.2, 0.])\nOBSTACLE_HALFDIMS = np.array([0.05, 0.25, 0.05])\nclass PandaPushingEnv(gym.Env): \n    def __init__(self, debug=False, visualizer=None, include_obstacle=True, render_non_push_motions=True,\n                 render_every_n_steps=1, camera_heigh=84, camera_width=84):\n        self.debug = debug",
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "BOX_SIZE",
        "kind": 5,
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "peekOfCode": "BOX_SIZE = 0.1\nTARGET_POSE_FREE = np.array([0.8, 0, 0.])\nTARGET_POSE_OBSTACLES = np.array([0.8, -0.1, 0.])\nOBSTACLE_CENTRE = np.array([0.6, 0.2, 0.])\nOBSTACLE_HALFDIMS = np.array([0.05, 0.25, 0.05])\nclass PandaPushingEnv(gym.Env): \n    def __init__(self, debug=False, visualizer=None, include_obstacle=True, render_non_push_motions=True,\n                 render_every_n_steps=1, camera_heigh=84, camera_width=84):\n        self.debug = debug\n        self.visualizer = visualizer",
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "TARGET_POSE_FREE",
        "kind": 5,
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "peekOfCode": "TARGET_POSE_FREE = np.array([0.8, 0, 0.])\nTARGET_POSE_OBSTACLES = np.array([0.8, -0.1, 0.])\nOBSTACLE_CENTRE = np.array([0.6, 0.2, 0.])\nOBSTACLE_HALFDIMS = np.array([0.05, 0.25, 0.05])\nclass PandaPushingEnv(gym.Env): \n    def __init__(self, debug=False, visualizer=None, include_obstacle=True, render_non_push_motions=True,\n                 render_every_n_steps=1, camera_heigh=84, camera_width=84):\n        self.debug = debug\n        self.visualizer = visualizer\n        self.include_obstacle = include_obstacle",
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "TARGET_POSE_OBSTACLES",
        "kind": 5,
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "peekOfCode": "TARGET_POSE_OBSTACLES = np.array([0.8, -0.1, 0.])\nOBSTACLE_CENTRE = np.array([0.6, 0.2, 0.])\nOBSTACLE_HALFDIMS = np.array([0.05, 0.25, 0.05])\nclass PandaPushingEnv(gym.Env): \n    def __init__(self, debug=False, visualizer=None, include_obstacle=True, render_non_push_motions=True,\n                 render_every_n_steps=1, camera_heigh=84, camera_width=84):\n        self.debug = debug\n        self.visualizer = visualizer\n        self.include_obstacle = include_obstacle\n        self.render_every_n_steps = render_every_n_steps",
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "OBSTACLE_CENTRE",
        "kind": 5,
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "peekOfCode": "OBSTACLE_CENTRE = np.array([0.6, 0.2, 0.])\nOBSTACLE_HALFDIMS = np.array([0.05, 0.25, 0.05])\nclass PandaPushingEnv(gym.Env): \n    def __init__(self, debug=False, visualizer=None, include_obstacle=True, render_non_push_motions=True,\n                 render_every_n_steps=1, camera_heigh=84, camera_width=84):\n        self.debug = debug\n        self.visualizer = visualizer\n        self.include_obstacle = include_obstacle\n        self.render_every_n_steps = render_every_n_steps\n        if debug:",
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "OBSTACLE_HALFDIMS",
        "kind": 5,
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "peekOfCode": "OBSTACLE_HALFDIMS = np.array([0.05, 0.25, 0.05])\nclass PandaPushingEnv(gym.Env): \n    def __init__(self, debug=False, visualizer=None, include_obstacle=True, render_non_push_motions=True,\n                 render_every_n_steps=1, camera_heigh=84, camera_width=84):\n        self.debug = debug\n        self.visualizer = visualizer\n        self.include_obstacle = include_obstacle\n        self.render_every_n_steps = render_every_n_steps\n        if debug:\n            p.connect(p.GUI)",
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "_EPS",
        "kind": 5,
        "importPath": "env.panda_pushing_env",
        "description": "env.panda_pushing_env",
        "peekOfCode": "_EPS = np.finfo(float).eps * 4.0\ndef quaternion_matrix(quaternion):\n    \"\"\"Return homogeneous rotation matrix from quaternion.\n1176\n1177      >>> R = quaternion_matrix([0.06146124, 0, 0, 0.99810947])\n1178      >>> numpy.allclose(R, rotation_matrix(0.123, (1, 0, 0)))\n1179      True\n1180\n1181      \"\"\"\n    q = np.array(quaternion[:4], dtype=np.float64, copy=True)",
        "detail": "env.panda_pushing_env",
        "documentation": {}
    },
    {
        "label": "GIFVisualizer",
        "kind": 6,
        "importPath": "gif.visualizers",
        "description": "gif.visualizers",
        "peekOfCode": "class GIFVisualizer(object):\n    def __init__(self):\n        self.frames = []\n    def set_data(self, img):\n        self.frames.append(img)\n    def reset(self):\n        self.frames = []\n    def get_gif(self):\n        # generate the gif\n        filename = 'gif/pushing_visualization.gif'",
        "detail": "gif.visualizers",
        "documentation": {}
    },
    {
        "label": "NotebookVisualizer",
        "kind": 6,
        "importPath": "gif.visualizers",
        "description": "gif.visualizers",
        "peekOfCode": "class NotebookVisualizer(object):\n    def __init__(self, fig, hfig):\n        self.fig = fig\n        self.hfig = hfig\n    def set_data(self, img):\n        plt.clf()\n        plt.imshow(img)\n        plt.axis('off')\n        self.fig.canvas.draw()\n        self.hfig.update(self.fig)",
        "detail": "gif.visualizers",
        "documentation": {}
    },
    {
        "label": "SE2PoseLoss",
        "kind": 6,
        "importPath": "model.losses",
        "description": "model.losses",
        "peekOfCode": "class SE2PoseLoss(nn.Module):\n    \"\"\"\n    Compute the SE2 pose loss based on the object dimensions (block_width, block_length).\n    Need to take into consideration the different dimensions of pose and orientation to aggregate them.\n    Given a SE(2) pose [x, y, theta], the pose loss can be computed as:\n        se2_pose_loss = MSE(x_hat, x) + MSE(y_hat, y) + rg * MSE(theta_hat, theta)\n    where rg is the radious of gyration of the object.\n    For a planar rectangular object of width w and length l, the radius of gyration is defined as:\n        rg = ((l^2 + w^2)/12)^{1/2}\n    \"\"\"",
        "detail": "model.losses",
        "documentation": {}
    },
    {
        "label": "SingleStepLoss",
        "kind": 6,
        "importPath": "model.losses",
        "description": "model.losses",
        "peekOfCode": "class SingleStepLoss(nn.Module):\n    def __init__(self, loss_fn):\n        super().__init__()\n        self.loss = loss_fn\n    def forward(self, model, state, action, target_state):\n        \"\"\"\n        Compute the single step loss resultant of querying model with (state, action) and comparing the predictions with target_state.\n        \"\"\"\n        single_step_loss = None\n        # --- Your code here",
        "detail": "model.losses",
        "documentation": {}
    },
    {
        "label": "MultiStepLoss",
        "kind": 6,
        "importPath": "model.losses",
        "description": "model.losses",
        "peekOfCode": "class MultiStepLoss(nn.Module):\n    def __init__(self, loss_fn, discount=0.99):\n        super().__init__()\n        self.loss = loss_fn\n        self.discount = discount\n    def forward(self, model, state, actions, target_states):\n        \"\"\"\n        Compute the multi-step loss resultant of multi-querying the model from (state, action) and comparing the predictions with targets.\n        \"\"\"\n        batch_size, num_steps = actions.shape[:2]",
        "detail": "model.losses",
        "documentation": {}
    },
    {
        "label": "ODEFunc",
        "kind": 6,
        "importPath": "model.neural_ODE_dynamics_models",
        "description": "model.neural_ODE_dynamics_models",
        "peekOfCode": "class ODEFunc(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n        self.state_dim = state_dim \n        self.action_dim = action_dim\n        self.net = nn.Sequential(\n            nn.Linear(self.state_dim + self.action_dim,100),\n            nn.ReLU(),\n            nn.Linear(100, 100),\n            nn.ReLU(),",
        "detail": "model.neural_ODE_dynamics_models",
        "documentation": {}
    },
    {
        "label": "NeuralODEDynamicsModel",
        "kind": 6,
        "importPath": "model.neural_ODE_dynamics_models",
        "description": "model.neural_ODE_dynamics_models",
        "peekOfCode": "class NeuralODEDynamicsModel(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.ode_func = ODEFunc(state_dim, action_dim)\n    def forward(self, state, action):\n        # Concatenate state and action to form augmented input\n        x_aug = torch.cat([state, action], dim=-1)  # (B, state+action)\n        t = torch.tensor([0, 1], dtype=torch.float32).to(state.device)  # Integration time",
        "detail": "model.neural_ODE_dynamics_models",
        "documentation": {}
    },
    {
        "label": "ResidualDynamicsModel",
        "kind": 6,
        "importPath": "model.nn_dynamics_models",
        "description": "model.nn_dynamics_models",
        "peekOfCode": "class ResidualDynamicsModel(nn.Module):\n    \"\"\"\n    Model the residual dynamics s_{t+1} = s_{t} + f(s_{t}, u_{t})\n    Observation: The network only needs to predict the state difference as a function of the state and action.\n    \"\"\"\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        # --- Your code here",
        "detail": "model.nn_dynamics_models",
        "documentation": {}
    },
    {
        "label": "N",
        "kind": 5,
        "importPath": "test.test_datasets",
        "description": "test.test_datasets",
        "peekOfCode": "N = 100 # Number of trajectories\nT = 10 # Trajectory length\ncollected_data = np.load('collected_data.npy', allow_pickle=True)\nenv = PandaPushingEnv()\nenv.reset()\nbatch_size = 500\ntrain_loader, val_loader = process_data_single_step(collected_data, batch_size=batch_size)\n# let's check your dataloader\n# you should return a dataloader\nprint('Is the returned train_loader a DataLoader?')",
        "detail": "test.test_datasets",
        "documentation": {}
    },
    {
        "label": "T",
        "kind": 5,
        "importPath": "test.test_datasets",
        "description": "test.test_datasets",
        "peekOfCode": "T = 10 # Trajectory length\ncollected_data = np.load('collected_data.npy', allow_pickle=True)\nenv = PandaPushingEnv()\nenv.reset()\nbatch_size = 500\ntrain_loader, val_loader = process_data_single_step(collected_data, batch_size=batch_size)\n# let's check your dataloader\n# you should return a dataloader\nprint('Is the returned train_loader a DataLoader?')\nprint('Yes' if isinstance(train_loader, torch.utils.data.DataLoader) else 'No')",
        "detail": "test.test_datasets",
        "documentation": {}
    },
    {
        "label": "collected_data",
        "kind": 5,
        "importPath": "test.test_datasets",
        "description": "test.test_datasets",
        "peekOfCode": "collected_data = np.load('collected_data.npy', allow_pickle=True)\nenv = PandaPushingEnv()\nenv.reset()\nbatch_size = 500\ntrain_loader, val_loader = process_data_single_step(collected_data, batch_size=batch_size)\n# let's check your dataloader\n# you should return a dataloader\nprint('Is the returned train_loader a DataLoader?')\nprint('Yes' if isinstance(train_loader, torch.utils.data.DataLoader) else 'No')\nprint('')",
        "detail": "test.test_datasets",
        "documentation": {}
    },
    {
        "label": "env",
        "kind": 5,
        "importPath": "test.test_datasets",
        "description": "test.test_datasets",
        "peekOfCode": "env = PandaPushingEnv()\nenv.reset()\nbatch_size = 500\ntrain_loader, val_loader = process_data_single_step(collected_data, batch_size=batch_size)\n# let's check your dataloader\n# you should return a dataloader\nprint('Is the returned train_loader a DataLoader?')\nprint('Yes' if isinstance(train_loader, torch.utils.data.DataLoader) else 'No')\nprint('')\n# You should have used random split to split your data - ",
        "detail": "test.test_datasets",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "test.test_datasets",
        "description": "test.test_datasets",
        "peekOfCode": "batch_size = 500\ntrain_loader, val_loader = process_data_single_step(collected_data, batch_size=batch_size)\n# let's check your dataloader\n# you should return a dataloader\nprint('Is the returned train_loader a DataLoader?')\nprint('Yes' if isinstance(train_loader, torch.utils.data.DataLoader) else 'No')\nprint('')\n# You should have used random split to split your data - \n# this means the validation and training sets are both subsets of an original dataset\nprint('Was random_split used to split the data?')",
        "detail": "test.test_datasets",
        "documentation": {}
    },
    {
        "label": "PushingController",
        "kind": 6,
        "importPath": "controller",
        "description": "controller",
        "peekOfCode": "class PushingController(object):\n    \"\"\"\n    MPPI-based controller\n    Since you implemented MPPI on HW2, here we will give you the MPPI for you.\n    You will just need to implement the dynamics and tune the hyperparameters and cost functions.\n    \"\"\"\n    def __init__(self, env, model, cost_function, num_samples=100, horizon=10):\n        self.env = env\n        self.model = model\n        self.target_state = None",
        "detail": "controller",
        "documentation": {}
    },
    {
        "label": "free_pushing_cost_function",
        "kind": 2,
        "importPath": "controller",
        "description": "controller",
        "peekOfCode": "def free_pushing_cost_function(state, action):\n    \"\"\"\n    Compute the state cost for MPPI on a setup without obstacles.\n    :param state: torch tensor of shape (B, state_size)\n    :param action: torch tensor of shape (B, state_size)\n    :return: cost: torch tensor of shape (B,) containing the costs for each of the provided states\n    \"\"\"\n    target_pose = TARGET_POSE_FREE_TENSOR  # torch tensor of shape (3,) containing (pose_x, pose_y, pose_theta)\n    cost = None\n    # --- Your code here",
        "detail": "controller",
        "documentation": {}
    },
    {
        "label": "collision_detection",
        "kind": 2,
        "importPath": "controller",
        "description": "controller",
        "peekOfCode": "def collision_detection(state):\n    \"\"\"\n    Checks if the state is in collision with the obstacle.\n    The obstacle geometry is known and provided in obstacle_centre and obstacle_halfdims.\n    :param state: torch tensor of shape (B, state_size)\n    :return: in_collision: torch tensor of shape (B,) containing 1 if the state is in collision and 0 if not.\n    \"\"\"\n    obstacle_centre = OBSTACLE_CENTRE_TENSOR  # torch tensor of shape (2,) consisting of obstacle centre (x, y)\n    obstacle_dims = 2 * OBSTACLE_HALFDIMS_TENSOR  # torch tensor of shape (2,) consisting of (w_obs, l_obs)\n    box_size = BOX_SIZE  # scalar for parameter w",
        "detail": "controller",
        "documentation": {}
    },
    {
        "label": "obstacle_avoidance_pushing_cost_function",
        "kind": 2,
        "importPath": "controller",
        "description": "controller",
        "peekOfCode": "def obstacle_avoidance_pushing_cost_function(state, action):\n    \"\"\"\n    Compute the state cost for MPPI on a setup with obstacles.\n    :param state: torch tensor of shape (B, state_size)\n    :param action: torch tensor of shape (B, state_size)\n    :return: cost: torch tensor of shape (B,) containing the costs for each of the provided states\n    \"\"\"\n    target_pose = TARGET_POSE_OBSTACLES_TENSOR  # torch tensor of shape (3,) containing (pose_x, pose_y, pose_theta)\n    cost = None\n    # --- Your code here",
        "detail": "controller",
        "documentation": {}
    },
    {
        "label": "TARGET_POSE_FREE_TENSOR",
        "kind": 5,
        "importPath": "controller",
        "description": "controller",
        "peekOfCode": "TARGET_POSE_FREE_TENSOR = torch.as_tensor(TARGET_POSE_FREE, dtype=torch.float32)\nTARGET_POSE_OBSTACLES_TENSOR = torch.as_tensor(TARGET_POSE_OBSTACLES, dtype=torch.float32)\nOBSTACLE_CENTRE_TENSOR = torch.as_tensor(OBSTACLE_CENTRE, dtype=torch.float32)[:2]\nOBSTACLE_HALFDIMS_TENSOR = torch.as_tensor(OBSTACLE_HALFDIMS, dtype=torch.float32)[:2]\ndef free_pushing_cost_function(state, action):\n    \"\"\"\n    Compute the state cost for MPPI on a setup without obstacles.\n    :param state: torch tensor of shape (B, state_size)\n    :param action: torch tensor of shape (B, state_size)\n    :return: cost: torch tensor of shape (B,) containing the costs for each of the provided states",
        "detail": "controller",
        "documentation": {}
    },
    {
        "label": "TARGET_POSE_OBSTACLES_TENSOR",
        "kind": 5,
        "importPath": "controller",
        "description": "controller",
        "peekOfCode": "TARGET_POSE_OBSTACLES_TENSOR = torch.as_tensor(TARGET_POSE_OBSTACLES, dtype=torch.float32)\nOBSTACLE_CENTRE_TENSOR = torch.as_tensor(OBSTACLE_CENTRE, dtype=torch.float32)[:2]\nOBSTACLE_HALFDIMS_TENSOR = torch.as_tensor(OBSTACLE_HALFDIMS, dtype=torch.float32)[:2]\ndef free_pushing_cost_function(state, action):\n    \"\"\"\n    Compute the state cost for MPPI on a setup without obstacles.\n    :param state: torch tensor of shape (B, state_size)\n    :param action: torch tensor of shape (B, state_size)\n    :return: cost: torch tensor of shape (B,) containing the costs for each of the provided states\n    \"\"\"",
        "detail": "controller",
        "documentation": {}
    },
    {
        "label": "OBSTACLE_CENTRE_TENSOR",
        "kind": 5,
        "importPath": "controller",
        "description": "controller",
        "peekOfCode": "OBSTACLE_CENTRE_TENSOR = torch.as_tensor(OBSTACLE_CENTRE, dtype=torch.float32)[:2]\nOBSTACLE_HALFDIMS_TENSOR = torch.as_tensor(OBSTACLE_HALFDIMS, dtype=torch.float32)[:2]\ndef free_pushing_cost_function(state, action):\n    \"\"\"\n    Compute the state cost for MPPI on a setup without obstacles.\n    :param state: torch tensor of shape (B, state_size)\n    :param action: torch tensor of shape (B, state_size)\n    :return: cost: torch tensor of shape (B,) containing the costs for each of the provided states\n    \"\"\"\n    target_pose = TARGET_POSE_FREE_TENSOR  # torch tensor of shape (3,) containing (pose_x, pose_y, pose_theta)",
        "detail": "controller",
        "documentation": {}
    },
    {
        "label": "OBSTACLE_HALFDIMS_TENSOR",
        "kind": 5,
        "importPath": "controller",
        "description": "controller",
        "peekOfCode": "OBSTACLE_HALFDIMS_TENSOR = torch.as_tensor(OBSTACLE_HALFDIMS, dtype=torch.float32)[:2]\ndef free_pushing_cost_function(state, action):\n    \"\"\"\n    Compute the state cost for MPPI on a setup without obstacles.\n    :param state: torch tensor of shape (B, state_size)\n    :param action: torch tensor of shape (B, state_size)\n    :return: cost: torch tensor of shape (B,) containing the costs for each of the provided states\n    \"\"\"\n    target_pose = TARGET_POSE_FREE_TENSOR  # torch tensor of shape (3,) containing (pose_x, pose_y, pose_theta)\n    cost = None",
        "detail": "controller",
        "documentation": {}
    },
    {
        "label": "SingleStepDynamicsDataset",
        "kind": 6,
        "importPath": "datasets",
        "description": "datasets",
        "peekOfCode": "class SingleStepDynamicsDataset(Dataset):\n    \"\"\"\n    Each data sample is a dictionary containing (x_t, u_t, x_{t+1}) in the form:\n    {'state': x_t,\n     'action': u_t,\n     'next_state': x_{t+1},\n    }\n    where:\n     x_t: torch.float32 tensor of shape (state_size,)\n     u_t: torch.float32 tensor of shape (action_size,)",
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "MultiStepDynamicsDataset",
        "kind": 6,
        "importPath": "datasets",
        "description": "datasets",
        "peekOfCode": "class MultiStepDynamicsDataset(Dataset):\n    \"\"\"\n    Dataset containing multi-step dynamics data.\n    Each data sample is a dictionary containing (state, action, next_state) in the form:\n    {'state': x_t, -- initial state of the multipstep torch.float32 tensor of shape (state_size,)\n     'action': [u_t,..., u_{t+num_steps-1}] -- actions applied in the muli-step.\n                torch.float32 tensor of shape (num_steps, action_size)\n     'next_state': [x_{t+1},..., x_{t+num_steps} ] -- next multiple steps for the num_steps next steps.\n                torch.float32 tensor of shape (num_steps, state_size)\n    }",
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "process_data_single_step",
        "kind": 2,
        "importPath": "datasets",
        "description": "datasets",
        "peekOfCode": "def process_data_single_step(collected_data, batch_size=500):\n    \"\"\"\n    Process the collected data and returns a DataLoader for train and one for validation.\n    The data provided is a list of trajectories (like collect_data_random output).\n    Each DataLoader must load dictionary as {'state': x_t,\n     'action': u_t,\n     'next_state': x_{t+1},\n    }\n    where:\n     x_t: torch.float32 tensor of shape (batch_size, state_size)",
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "process_data_multiple_step",
        "kind": 2,
        "importPath": "datasets",
        "description": "datasets",
        "peekOfCode": "def process_data_multiple_step(collected_data, batch_size=500, num_steps=4):\n    \"\"\"\n    Process the collected data and returns a DataLoader for train and one for validation.\n    The data provided is a list of trajectories (like collect_data_random output).\n    Each DataLoader must load dictionary as\n    {'state': x_t,\n     'action': u_t, ..., u_{t+num_steps-1},\n     'next_state': x_{t+1}, ... , x_{t+num_steps}\n    }\n    where:",
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "eval",
        "description": "eval",
        "peekOfCode": "MODEL = \"neural_ode_dynamics_model\"\nMAX_STEPS = 20\nNUM_STEPS_TRAING = 4\n# init visualizer\nfig = plt.figure(figsize=(8,8))\nhfig = display(fig, display_id=True)\nvisualizer = GIFVisualizer()\n# load the model\nenv = PandaPushingEnv(visualizer=visualizer, render_non_push_motions=True,  camera_heigh=800, camera_width=800)\npushing_model = None",
        "detail": "eval",
        "documentation": {}
    },
    {
        "label": "MAX_STEPS",
        "kind": 5,
        "importPath": "eval",
        "description": "eval",
        "peekOfCode": "MAX_STEPS = 20\nNUM_STEPS_TRAING = 4\n# init visualizer\nfig = plt.figure(figsize=(8,8))\nhfig = display(fig, display_id=True)\nvisualizer = GIFVisualizer()\n# load the model\nenv = PandaPushingEnv(visualizer=visualizer, render_non_push_motions=True,  camera_heigh=800, camera_width=800)\npushing_model = None\nif MODEL == \"neural_ode_dynamics_model\":",
        "detail": "eval",
        "documentation": {}
    },
    {
        "label": "NUM_STEPS_TRAING",
        "kind": 5,
        "importPath": "eval",
        "description": "eval",
        "peekOfCode": "NUM_STEPS_TRAING = 4\n# init visualizer\nfig = plt.figure(figsize=(8,8))\nhfig = display(fig, display_id=True)\nvisualizer = GIFVisualizer()\n# load the model\nenv = PandaPushingEnv(visualizer=visualizer, render_non_push_motions=True,  camera_heigh=800, camera_width=800)\npushing_model = None\nif MODEL == \"neural_ode_dynamics_model\":\n    pushing_neural_ode_dynamics_model = NeuralODEDynamicsModel(",
        "detail": "eval",
        "documentation": {}
    },
    {
        "label": "fig",
        "kind": 5,
        "importPath": "eval",
        "description": "eval",
        "peekOfCode": "fig = plt.figure(figsize=(8,8))\nhfig = display(fig, display_id=True)\nvisualizer = GIFVisualizer()\n# load the model\nenv = PandaPushingEnv(visualizer=visualizer, render_non_push_motions=True,  camera_heigh=800, camera_width=800)\npushing_model = None\nif MODEL == \"neural_ode_dynamics_model\":\n    pushing_neural_ode_dynamics_model = NeuralODEDynamicsModel(\n        env.observation_space.shape[0],\n        env.action_space.shape[0]",
        "detail": "eval",
        "documentation": {}
    },
    {
        "label": "hfig",
        "kind": 5,
        "importPath": "eval",
        "description": "eval",
        "peekOfCode": "hfig = display(fig, display_id=True)\nvisualizer = GIFVisualizer()\n# load the model\nenv = PandaPushingEnv(visualizer=visualizer, render_non_push_motions=True,  camera_heigh=800, camera_width=800)\npushing_model = None\nif MODEL == \"neural_ode_dynamics_model\":\n    pushing_neural_ode_dynamics_model = NeuralODEDynamicsModel(\n        env.observation_space.shape[0],\n        env.action_space.shape[0]\n    )",
        "detail": "eval",
        "documentation": {}
    },
    {
        "label": "visualizer",
        "kind": 5,
        "importPath": "eval",
        "description": "eval",
        "peekOfCode": "visualizer = GIFVisualizer()\n# load the model\nenv = PandaPushingEnv(visualizer=visualizer, render_non_push_motions=True,  camera_heigh=800, camera_width=800)\npushing_model = None\nif MODEL == \"neural_ode_dynamics_model\":\n    pushing_neural_ode_dynamics_model = NeuralODEDynamicsModel(\n        env.observation_space.shape[0],\n        env.action_space.shape[0]\n    )\n    state_dict = torch.load(f\"checkpoint/pushing_{NUM_STEPS_TRAING}_steps_ode_dynamics_model.pt\")",
        "detail": "eval",
        "documentation": {}
    },
    {
        "label": "env",
        "kind": 5,
        "importPath": "eval",
        "description": "eval",
        "peekOfCode": "env = PandaPushingEnv(visualizer=visualizer, render_non_push_motions=True,  camera_heigh=800, camera_width=800)\npushing_model = None\nif MODEL == \"neural_ode_dynamics_model\":\n    pushing_neural_ode_dynamics_model = NeuralODEDynamicsModel(\n        env.observation_space.shape[0],\n        env.action_space.shape[0]\n    )\n    state_dict = torch.load(f\"checkpoint/pushing_{NUM_STEPS_TRAING}_steps_ode_dynamics_model.pt\")\n    pushing_neural_ode_dynamics_model.load_state_dict(state_dict)\n    pushing_model = pushing_neural_ode_dynamics_model.eval()",
        "detail": "eval",
        "documentation": {}
    },
    {
        "label": "pushing_model",
        "kind": 5,
        "importPath": "eval",
        "description": "eval",
        "peekOfCode": "pushing_model = None\nif MODEL == \"neural_ode_dynamics_model\":\n    pushing_neural_ode_dynamics_model = NeuralODEDynamicsModel(\n        env.observation_space.shape[0],\n        env.action_space.shape[0]\n    )\n    state_dict = torch.load(f\"checkpoint/pushing_{NUM_STEPS_TRAING}_steps_ode_dynamics_model.pt\")\n    pushing_neural_ode_dynamics_model.load_state_dict(state_dict)\n    pushing_model = pushing_neural_ode_dynamics_model.eval()\nelif MODEL == \"residual_dynamics_model\":",
        "detail": "eval",
        "documentation": {}
    },
    {
        "label": "env",
        "kind": 5,
        "importPath": "eval",
        "description": "eval",
        "peekOfCode": "env = PandaPushingEnv(visualizer=visualizer, render_non_push_motions=False,  camera_heigh=800, camera_width=800, render_every_n_steps=5)\n# load pushing controller - mppi\ncontroller = PushingController(env, pushing_model, free_pushing_cost_function, num_samples=100, horizon=10)\n# test the model\nstate = env.reset()\nstep = 0\nfor i in tqdm(range(MAX_STEPS)):\n    action = controller.control(state)\n    state, reward, done, _ = env.step(action)\n    step += 1",
        "detail": "eval",
        "documentation": {}
    },
    {
        "label": "controller",
        "kind": 5,
        "importPath": "eval",
        "description": "eval",
        "peekOfCode": "controller = PushingController(env, pushing_model, free_pushing_cost_function, num_samples=100, horizon=10)\n# test the model\nstate = env.reset()\nstep = 0\nfor i in tqdm(range(MAX_STEPS)):\n    action = controller.control(state)\n    state, reward, done, _ = env.step(action)\n    step += 1\n    if done:\n        break",
        "detail": "eval",
        "documentation": {}
    },
    {
        "label": "state",
        "kind": 5,
        "importPath": "eval",
        "description": "eval",
        "peekOfCode": "state = env.reset()\nstep = 0\nfor i in tqdm(range(MAX_STEPS)):\n    action = controller.control(state)\n    state, reward, done, _ = env.step(action)\n    step += 1\n    if done:\n        break\n# evaluate if goal is reached\nend_state = env.get_state()",
        "detail": "eval",
        "documentation": {}
    },
    {
        "label": "step",
        "kind": 5,
        "importPath": "eval",
        "description": "eval",
        "peekOfCode": "step = 0\nfor i in tqdm(range(MAX_STEPS)):\n    action = controller.control(state)\n    state, reward, done, _ = env.step(action)\n    step += 1\n    if done:\n        break\n# evaluate if goal is reached\nend_state = env.get_state()\ntarget_state = TARGET_POSE_FREE",
        "detail": "eval",
        "documentation": {}
    },
    {
        "label": "end_state",
        "kind": 5,
        "importPath": "eval",
        "description": "eval",
        "peekOfCode": "end_state = env.get_state()\ntarget_state = TARGET_POSE_FREE\ngoal_distance = np.linalg.norm(end_state[:2]-target_state[:2]) \ngoal_reached = goal_distance < BOX_SIZE\nprint(f'GOAL REACHED: {goal_reached}')\nprint(f'GOAL DISTANCE: {goal_distance:.2f}')\nprint(f'NUMBER OF STEPS: {step}')\nvisualizer.get_gif() \nplt.close(fig)",
        "detail": "eval",
        "documentation": {}
    },
    {
        "label": "target_state",
        "kind": 5,
        "importPath": "eval",
        "description": "eval",
        "peekOfCode": "target_state = TARGET_POSE_FREE\ngoal_distance = np.linalg.norm(end_state[:2]-target_state[:2]) \ngoal_reached = goal_distance < BOX_SIZE\nprint(f'GOAL REACHED: {goal_reached}')\nprint(f'GOAL DISTANCE: {goal_distance:.2f}')\nprint(f'NUMBER OF STEPS: {step}')\nvisualizer.get_gif() \nplt.close(fig)",
        "detail": "eval",
        "documentation": {}
    },
    {
        "label": "goal_distance",
        "kind": 5,
        "importPath": "eval",
        "description": "eval",
        "peekOfCode": "goal_distance = np.linalg.norm(end_state[:2]-target_state[:2]) \ngoal_reached = goal_distance < BOX_SIZE\nprint(f'GOAL REACHED: {goal_reached}')\nprint(f'GOAL DISTANCE: {goal_distance:.2f}')\nprint(f'NUMBER OF STEPS: {step}')\nvisualizer.get_gif() \nplt.close(fig)",
        "detail": "eval",
        "documentation": {}
    },
    {
        "label": "goal_reached",
        "kind": 5,
        "importPath": "eval",
        "description": "eval",
        "peekOfCode": "goal_reached = goal_distance < BOX_SIZE\nprint(f'GOAL REACHED: {goal_reached}')\nprint(f'GOAL DISTANCE: {goal_distance:.2f}')\nprint(f'NUMBER OF STEPS: {step}')\nvisualizer.get_gif() \nplt.close(fig)",
        "detail": "eval",
        "documentation": {}
    },
    {
        "label": "MPPI",
        "kind": 6,
        "importPath": "mppi",
        "description": "mppi",
        "peekOfCode": "class MPPI():\n    \"\"\"\n    Model Predictive Path Integral control\n    This implementation batch samples the trajectories and so scales well with the number of samples K.\n    Implemented according to algorithm 2 in Williams et al., 2017\n    'Information Theoretic MPC for Model-Based Reinforcement Learning',\n    based off of https://github.com/ferreirafabio/mppi_pendulum\n    \"\"\"\n    def __init__(self, dynamics, running_cost, nx, noise_sigma, num_samples=100, horizon=15, device=\"cpu\",\n                 terminal_state_cost=None,",
        "detail": "mppi",
        "documentation": {}
    },
    {
        "label": "is_tensor_like",
        "kind": 2,
        "importPath": "mppi",
        "description": "mppi",
        "peekOfCode": "def is_tensor_like(x):\n    return torch.is_tensor(x) or type(x) is np.ndarray\ndef squeeze_n(v, n_squeeze):\n    for _ in range(n_squeeze):\n        v = v.squeeze(0)\n    return v\n# from arm_pytorch_utilities, standalone since that package is not on pypi yet\ndef handle_batch_input(n):\n    def _handle_batch_input(func):\n        \"\"\"For func that expect 2D input, handle input that have more than 2 dimensions by flattening them temporarily\"\"\"",
        "detail": "mppi",
        "documentation": {}
    },
    {
        "label": "squeeze_n",
        "kind": 2,
        "importPath": "mppi",
        "description": "mppi",
        "peekOfCode": "def squeeze_n(v, n_squeeze):\n    for _ in range(n_squeeze):\n        v = v.squeeze(0)\n    return v\n# from arm_pytorch_utilities, standalone since that package is not on pypi yet\ndef handle_batch_input(n):\n    def _handle_batch_input(func):\n        \"\"\"For func that expect 2D input, handle input that have more than 2 dimensions by flattening them temporarily\"\"\"\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):",
        "detail": "mppi",
        "documentation": {}
    },
    {
        "label": "handle_batch_input",
        "kind": 2,
        "importPath": "mppi",
        "description": "mppi",
        "peekOfCode": "def handle_batch_input(n):\n    def _handle_batch_input(func):\n        \"\"\"For func that expect 2D input, handle input that have more than 2 dimensions by flattening them temporarily\"\"\"\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # assume inputs that are tensor-like have compatible shapes and is represented by the first argument\n            batch_dims = []\n            for arg in args:\n                if is_tensor_like(arg):\n                    if len(arg.shape) > n:",
        "detail": "mppi",
        "documentation": {}
    },
    {
        "label": "run_mppi",
        "kind": 2,
        "importPath": "mppi",
        "description": "mppi",
        "peekOfCode": "def run_mppi(mppi, env, retrain_dynamics, retrain_after_iter=50, iter=1000, render=True):\n    dataset = torch.zeros((retrain_after_iter, mppi.nx + mppi.nu), dtype=mppi.U.dtype, device=mppi.d)\n    total_reward = 0\n    for i in range(iter):\n        state = env.state.copy()\n        command_start = time.perf_counter()\n        action = mppi.command(state)\n        elapsed = time.perf_counter() - command_start\n        s, r, _, _ = env.step(action.cpu().numpy())\n        total_reward += r",
        "detail": "mppi",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "mppi",
        "description": "mppi",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef _ensure_non_zero(cost, beta, factor):\n    return torch.exp(-factor * (cost - beta))\ndef is_tensor_like(x):\n    return torch.is_tensor(x) or type(x) is np.ndarray\ndef squeeze_n(v, n_squeeze):\n    for _ in range(n_squeeze):\n        v = v.squeeze(0)\n    return v\n# from arm_pytorch_utilities, standalone since that package is not on pypi yet",
        "detail": "mppi",
        "documentation": {}
    },
    {
        "label": "train_step",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def train_step(model, train_loader, optimizer, loss_fn) -> float:\n    \"\"\"\n    Performs an epoch train step.\n    :param model: Pytorch nn.Module\n    :param train_loader: Pytorch DataLoader\n    :param optimizer: Pytorch optimizer\n    :param loss_fn: The loss function to use.\n    :return: train_loss <float> representing the average loss among the different mini-batches.\n    \"\"\"\n    train_loss = 0.",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "val_step",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def val_step(model, val_loader, loss_fn) -> float:\n    \"\"\"\n    Performs an epoch of model performance validation\n    :param model: Pytorch nn.Module\n    :param val_loader: Pytorch DataLoader\n    :param loss_fn: The loss function to use.\n    :return: val_loss <float> representing the average loss among the different mini-batches\n    \"\"\"\n    val_loss = 0.\n    model.eval()",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def train_model(model, train_dataloader, val_dataloader, loss_fn, save_path, num_epochs=100, lr=1e-3):\n    \"\"\"\n    Trains the given model for `num_epochs` epochs. Uses Adam optimizer.\n    Saves the best model based on validation loss.\n    Creates the save directory if it doesn't exist.\n    :param model: Pytorch nn.Module.\n    :param train_dataloader: Pytorch DataLoader with the training data.\n    :param val_dataloader: Pytorch DataLoader with the validation data.\n    :param loss_fn: The loss function to use.\n    :param save_path: Path to save the best model checkpoint.",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "MODEL = \"residual\" \n# MODEL = \"ode\"\nLR = 0.0001\nNUM_EPOCHS = 3000\nNUM_STEPS = 4\nBATCH_SIZE = 500\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nCHECKPOINT_DIR = \"checkpoint\" \nPLOT_DIR = \"media/plots\"\ndef train_step(model, train_loader, optimizer, loss_fn) -> float:",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "LR",
        "kind": 5,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "LR = 0.0001\nNUM_EPOCHS = 3000\nNUM_STEPS = 4\nBATCH_SIZE = 500\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nCHECKPOINT_DIR = \"checkpoint\" \nPLOT_DIR = \"media/plots\"\ndef train_step(model, train_loader, optimizer, loss_fn) -> float:\n    \"\"\"\n    Performs an epoch train step.",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "NUM_EPOCHS",
        "kind": 5,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "NUM_EPOCHS = 3000\nNUM_STEPS = 4\nBATCH_SIZE = 500\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nCHECKPOINT_DIR = \"checkpoint\" \nPLOT_DIR = \"media/plots\"\ndef train_step(model, train_loader, optimizer, loss_fn) -> float:\n    \"\"\"\n    Performs an epoch train step.\n    :param model: Pytorch nn.Module",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "NUM_STEPS",
        "kind": 5,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "NUM_STEPS = 4\nBATCH_SIZE = 500\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nCHECKPOINT_DIR = \"checkpoint\" \nPLOT_DIR = \"media/plots\"\ndef train_step(model, train_loader, optimizer, loss_fn) -> float:\n    \"\"\"\n    Performs an epoch train step.\n    :param model: Pytorch nn.Module\n    :param train_loader: Pytorch DataLoader",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "BATCH_SIZE",
        "kind": 5,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "BATCH_SIZE = 500\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nCHECKPOINT_DIR = \"checkpoint\" \nPLOT_DIR = \"media/plots\"\ndef train_step(model, train_loader, optimizer, loss_fn) -> float:\n    \"\"\"\n    Performs an epoch train step.\n    :param model: Pytorch nn.Module\n    :param train_loader: Pytorch DataLoader\n    :param optimizer: Pytorch optimizer",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "DEVICE",
        "kind": 5,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nCHECKPOINT_DIR = \"checkpoint\" \nPLOT_DIR = \"media/plots\"\ndef train_step(model, train_loader, optimizer, loss_fn) -> float:\n    \"\"\"\n    Performs an epoch train step.\n    :param model: Pytorch nn.Module\n    :param train_loader: Pytorch DataLoader\n    :param optimizer: Pytorch optimizer\n    :param loss_fn: The loss function to use.",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "CHECKPOINT_DIR",
        "kind": 5,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "CHECKPOINT_DIR = \"checkpoint\" \nPLOT_DIR = \"media/plots\"\ndef train_step(model, train_loader, optimizer, loss_fn) -> float:\n    \"\"\"\n    Performs an epoch train step.\n    :param model: Pytorch nn.Module\n    :param train_loader: Pytorch DataLoader\n    :param optimizer: Pytorch optimizer\n    :param loss_fn: The loss function to use.\n    :return: train_loss <float> representing the average loss among the different mini-batches.",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "PLOT_DIR",
        "kind": 5,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "PLOT_DIR = \"media/plots\"\ndef train_step(model, train_loader, optimizer, loss_fn) -> float:\n    \"\"\"\n    Performs an epoch train step.\n    :param model: Pytorch nn.Module\n    :param train_loader: Pytorch DataLoader\n    :param optimizer: Pytorch optimizer\n    :param loss_fn: The loss function to use.\n    :return: train_loss <float> representing the average loss among the different mini-batches.\n    \"\"\"",
        "detail": "train",
        "documentation": {}
    }
]